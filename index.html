<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Project page of the paper 'VRMM: A Volumetric Relightable Morphable Head Model'">
  <meta property="og:title" content="VRMM" />
  <meta property="og:description"
    content="Project page of the paper 'VRMM: A Volumetric Relightable Morphable Head Model'" />
  <meta property="og:url" content="VRMM-paper.github.io" />
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/images/fig_representative.jpg" />
  <meta property="og:image:width" content="1500" />
  <meta property="og:image:height" content="1000" />


  <meta name="twitter:title" content="VRMM">
  <meta name="twitter:description"
    content="Project page of the paper 'VRMM: A Volumetric Relightable Morphable Head Model'">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/fig_representative.jpg">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>VRMM: A Volumetric Relightable Morphable Head Model</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<style>
  .method-img {
    width: 80%;
    margin-left: auto;
    margin-right: auto;
    display: block;
  }

  .text-center {
    text-align: center;
  }
</style>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">VRMM: A Volumetric Relightable Morphable Head Model</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://yanght321.github.io/" target="_blank">Haotian Yang</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=MdizB60AAAAJ&hl=en" target="_blank">Mingwu
                  Zheng</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="http://www.chongyangma.com/" target="_blank">Chongyang Ma</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://users.cs.cf.ac.uk/Yukun.Lai/" target="_blank">Yu-Kun Lai</a><sup>2</sup>,</span>
              <span class="author-block"></span>
              <a href="https://scholar.google.com/citations?user=P6MraaYAAAAJ&hl=en" target="_blank">Pengfei
                Wan</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://brotherhuang.github.io/" target="_blank">Haibin Huang</a><sup>1</sup>,</span>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>Kuaishou Technology <sup>2</sup>Cardiff University<br>SIGGRAPH
                2024</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- Arxiv PDF link -->
                <!-- <span class="link-block">
                                    <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                                    class="external-link button is-normal is-rounded is-dark">
                                    <span class="icon">
                                    <i class="fas fa-file-pdf"></i>
                                    </span>
                                    <span>Paper</span>
                                </a>
                                </span> -->

                <!-- Supplementary PDF link -->
                <!-- <span class="link-block">
                                <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                                class="external-link button is-normal is-rounded is-dark">
                                <span class="icon">
                                    <i class="fas fa-file-pdf"></i>
                                </span>
                                <span>Supplementary</span>
                                </a> -->
                </span>

                <!-- Github link -->
                <span class="link-block">
                  <a href="https://github.com/YOUR REPO HERE" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code (comimg soon!)</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2402.04101" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
    </div>
    </div>
    </div>
  </section>

  <!-- Image carousel -->
  <section class="hero is-small">
    <div class="hero-body">
      <div class="container">
        <!-- class="carousel results-carousel" -->
        <div id="results-carousel">
          <div class="item">
            <!-- Your image here -->
            <img src="static/images/fig_teaser.png" alt="MY ALT TEXT" />
            <h2 class="subtitle has-text-centered">
              We present VRMM, a novel volumetric head prior with fully disentangled low-dimensional parametric space
              for identity, expression, and illumination. Trained on dynamic expressions of hundreds of people captured
              in a LightStage with controllable illumination, our VRMM enables high-quality animatable and relightable
              avatar reconstruction from few-shot observations.
            </h2>
            </h2>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End image carousel -->

  <!-- Paper abstract -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              In this paper, we introduce the Volumetric Relightable Morphable Model (VRMM), a novel volumetric and
              parametric facial prior for 3D face modeling. While recent volumetric prior models offer improvements over
              traditional methods like 3D Morphable Models (3DMMs), they face challenges in model learning and
              personalized reconstructions. Our VRMM overcomes these by employing a novel training framework that
              efficiently disentangles and encodes latent spaces of identity, expression, and lighting into
              low-dimensional representations. This framework, designed with self-supervised learning, significantly
              reduces the constraints for training data, making it more feasible in practice. The learned VRMM offers
              relighting capabilities and encompasses a comprehensive range of expressions. We demonstrate the
              versatility and effectiveness of VRMM through various applications like avatar generation, facial
              reconstruction, and animation. Additionally, we address the common issue of overfitting in generative
              volumetric models with a novel prior-preserving personalization framework based on VRMM. Such an approach
              enables high-quality 3D face reconstruction from even a single portrait input. Our experiments showcase the
              potential of VRMM to significantly enhance the field of 3D face modeling.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End paper abstract -->


  <!-- Image carousel -->
  <section class="hero is-small">
    <div class="hero-body">
      <div class="container">
        <div id="results-carousel">
          <div class="item">
            <!-- Your image here -->
            <img class="method-img" src="static/images/fig_method.png" alt="MY ALT TEXT" />
            <h2 class="subtitle has-text-centered">
              The VRMM pipeline. Network architecture (left): VRMM accepts inputs of identity code, expression code,
              view direction, and environmental light. The output, comprising a base mesh and volumetric primitives, is
              generated by respective decoders and rendered into an image in real-time. Notably, the transformation
              decoder, opacity decoder, and the non-linear branch of the relightable appearance decoder are
              interconnected through a detach-concatenation process between blocks, a key factor we found for achieving
              stable results. Training Framework (right): Our framework jointly trains the expression encoder,
              transformation encoder, per-person identity codes, and the decoders in VRMM. Additionally, we incorporate
              a novel expression consistency loss to enhance the semantic alignment of expression codes.
            </h2>
            </h2>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End image carousel -->

  <section class="hero is-small is-light">
    <div class="hero-body">
      <div class="container">
        <!-- Paper video. -->
        <h2 class="title is-3 text-center">Video</h2>
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">

            <div class="publication-video">
              <!-- Youtube embed code here -->
              <iframe src="https://www.youtube.com/embed/Q-i0-S2mjRc?si=8IS-6_9QdqaLjgrh" frameborder="0"
                allow="autoplay; encrypted-media" allowfullscreen></iframe>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
        @inproceedings{yang2024vrmm,
            title={VRMM: A Volumetric Relightable Morphable Head Model},
            author={Haotian, Yang and Mingwu, Zheng and ChongYang, Ma and Yu-Kun, Lai and Pengfei, Wan and Haibin, Huang},
            booktitle={SIGGRAPH 2024 Conference Proceedings},
            year={2024}
        }</code></pre>
    </div>
  </section>
  <!--End BibTex citation -->


  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">

            <p>
              This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                target="_blank">Academic Project Page Template</a>.
              <br> This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>

          </div>
        </div>
      </div>
    </div>
  </footer>

  <!-- Statcounter tracking code -->

  <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

  <!-- End of Statcounter Code -->

</body>

</html>
